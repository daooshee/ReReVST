<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>ReReVST</title>
    <style type="text/css">
        body{
        	background-color: white;
        }
        .links{
        	text-decoration: none;
        	color: blue;
        }
        .p2{
        	padding-top: 20px;
        	font-size: 25px;
        	line-height: 1.5;
        }
        .p1{
        	text-align:justify;
        	text-justify:inter-ideograph;
        	line-height: 1.2;
        }
        a{
        	font-family: Sans-serif;
        }
        p{
        	font-family: Sans-serif;
        }
        ul{
        	font-family: Sans-serif;
        }
    </style>
</head>
<body>
	<div align="center" style="padding-top: 30px;">
	<p>TIP 2020</p>
	<p style="font-size:35px;">Consistent Video Style Transfer via Relaxation and Regularization</p>
	<table style="width: 600px">
		<tr>
			<td>
				<a href="https://daooshee.github.io/website/" class="links">Wenjing Wang</a>
			</td>
			<td>
				<a href="https://williamyang1991.github.io" class="links">Shuai Yang</a>
			</td>
			<td>
				<a href="https://ieeexplore.ieee.org/author/37279201100" class="links">Jizheng Xu</a>
			</td>
			<td>
				<a href="http://www.icst.pku.edu.cn/struct/people/liujiaying.html" class="links">Jiaying Liu</a>
			</td>
		</tr>
	</table>
	</div>

	<div align="left" style="padding-left: 200px; padding-right: 200px; padding-bottom: 20px;">

	<p class='p2'> Abstract </p> 
	<p class='p1'> In recent years, neural style transfer has attracted more and more attention, especially for image style transfer. However, temporally consistent style transfer for videos is still a challenging problem. Existing methods, either relying on a significant amount of video data with optical flows or using single-frame regularizers, fail to handle strong motions or complex variations, therefore have limited performance on real videos. We first identify the cause of the conflict between style transfer and temporal consistency, and propose to reconcile this contradiction by relaxing the objective function, so as to make the stylization loss term more robust to motions. Through relaxation, style transfer is more robust to inter-frame variation without degrading the subjective effect. Based on relaxation and regularization, we design a zero-shot video style transfer framework. Moreover, for better feature migration, we introduce a new module to dynamically adjust inter-channel distributions. Quantitative and qualitative results demonstrate the superiority of our method over state-of-the-art style transfer methods. </p>

	<p class='p2'> Consistent Video Style Transfer </p> 
	<div align="center">
		<img src="architecture.jpg" width=100%> <br>
	</div>
	<p class='p1'> Figure 1. Left: the proposed decorator block for inter-channel feature adjustment. Both target style features and input content features are fed into a shallow sub-network Filter Predictor to predict filters. Residual learning and dimensionality reduction are used to improve the efficiency. Right: The overall architecture of the proposed encoder-decoder style transfer network. </p>

	<p class='p2'> Resources </p> 
	<p class='p1'>
		<ul style="line-height:2">
 			<li> Paper: <a href="https://ieeexplore.ieee.org/document/9204808" class="links">IEEE Xplore</a></li>
			<li> Code: <a href="https://github.com/daooshee/ReReVST-Code" class="links">Github</a></li>
			<li> Supplementary (PDF+Video): <a href="https://drive.google.com/drive/folders/1RSmjqZTon3QdxBUSjZ3siGIOwUc-Ycu8?usp=sharing" class="links">Google Drive</a>, <a href="https://pan.baidu.com/s/1Td30bukn2nc4zepmSDs1mA" class="links">Baidu Pan</a> [397w]</li></li>
		</ul>
	</p>

	<p class='p2'> Citation</p>
	<p> @article{ReReVST2020, <br>
		&nbsp; &nbsp; author = {Wang, Wenjing and Yang, Shuai and Xu, Jizheng and Liu, Jiaying}, <br>
		&nbsp; &nbsp; title = {Consistent Video Style Transfer via Relaxation and Regularization}, <br>
		&nbsp; &nbsp; journal = {{IEEE} Trans. Image Process.}, <br>
	   	&nbsp; &nbsp; year = {2020} <br>
	} <br> 
	</p>

	<p class='p2'> Selected Results </p> 
	<div align="center">
		<img src="compare_result.jpg" width="80%">
	</div>
	<p class='p1'> Figure 2. Quantitative evaluation of temporal consistency. Our model yields the lowest temporal loss for all temporal length.</p>

	<br>

	<div align="center">
		<img src="compare_result_video.jpg" width="80%">
	</div>
	<p class='p1'> Figure 3. Comparisons on video style transfer. The bottom of each row shows the temporal error heat map. Our method can best balance stylization and maintaining temporal consistency.</p>

	<br>

	<div align="center">
		<img src="compare_result_video2.jpg" width="80%">
	</div>
	<p class='p1'> Figure 4. More comparison results on special cases of (a) changing focus, (b) changing illumination, (c) large motion. Compared with Ruder et al. [1], the result of our model fully reflects the content of the input video. Besides, the texture and color of our result are more consistent with the input style image.</p>

	<br>

	<div align="center">
		<img src="compare_aaai.jpg" width="50%">
	</div>
	<p class='p1'> Figure 5. Qualitative comparison against our earlier publication [2]. With the proposed relaxed style loss, the color becomes richer, the details are clearer, and the texture of strokes is closer to the style reference. This demonstrates the effectiveness of our new style loss relaxation scheme.</p>

	<p class='p2'> Related Project</p>
	<p class='p1'> <a href="https://daooshee.github.io/CompoundVST/"> Consistent Video Style Transfer via Compound Regularization</a> </p>

	<p class='p2'> More</p>
	<p class='p1'>Our project is used in two songs of the amazing <a href="https://daooshee.github.io/CompoundVST/">Sound Escapes</a> online concert.</p> 

	<p class='p2'> Reference</p>
	<p class='p1'> [1] M. Ruder, A. Dosovitskiy, and T. Brox, “Artistic style transfer for videos,” in Proc. German Conference on Pattern Recognition, 2016, pp. 26–36.</p>

	<p class='p1'> [2] W. Wang, J. Xu, L. Zhang, Y. Wang, and J. Liu, “Consistent video style transfer via compound regularization,” in Proc. AAAI Conference on Artificial Intelligence, 2020, pp. 12 233–12 240.</p>

</html>
