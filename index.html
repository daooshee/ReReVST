<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>ReReVST</title>
    <style type="text/css">
        body{
        	background-color: white;
        }
        .links{
        	text-decoration: none;
        	color: blue;
        }
        .p2{
        	padding-top: 20px;
        	font-size: 25px;
        }
        .p1{
        	text-align:justify;
        	text-justify:inter-ideograph;
        }
        a{
        	font-family: Sans-serif;
        }
        p{
        	font-family: Sans-serif;
        }
        ul{
        	font-family: Sans-serif;
        }
    </style>
</head>
<body>
	<div align="center" style="padding-top: 30px;">
	<p>Accepted by TIP 2020</p>
	<p style="font-size:35px;">Consistent Video Style Transfer via Relaxation and Regularization</p>
	<table style="width: 600px">
		<tr>
			<td>
				<a href="mailto:daooshee@pku.edu.cn" class="links">Wenjing Wang</a>
			</td>
			<td>
				<a href="mailto:williamyang@pku.edu.cn" class="links">Shuai Yang</a>
			</td>
			<td>
				<a href="mailto:xujizheng@bytedance.com" class="links">Jizheng Xu</a>
			</td>
			<td>
				<a href="mailto:liujiaying@pku.edu.cn" class="links">Jiaying Liu</a>
			</td>
		</tr>
	</table>
	</div>

	<div align="left" style="padding-left: 200px; padding-right: 200px; padding-bottom: 20px;">

<!-- 	<p class='p2'> </p> 
	<div align="center">
		<img src="1.gif" alt="this slowpoke moves" height=200px>
		<img src="2.gif" alt="this slowpoke moves" height=200px>
		<img src="3.gif" alt="this slowpoke moves" height=200px>
		<img src="4.gif" alt="this slowpoke moves" height=200px>
	</div>
	<div style="padding-left: 60px; padding-right: 60px;">
	<p class='p1'> Figure 1. We propose a novel video style transfer framework, which can produce temporally consistent results and is highly robust to intense object movements and illumination changes. Furthermore, benefiting from the nice properties of our framework and model, we can enable features that traditional optical-flow-based methods cannot provide, such as dynamically changing styles over time.
	</p>
	</div> -->

	<p class='p2'> Abstract </p> 
	<p class='p1'> In recent years, neural style transfer has attracted more and more attention, especially for image style transfer. However, temporally consistent style transfer for videos is still a challenging problem. Existing methods, either relying on a significant amount of video data with optical flows or using single-frame regularizers, fail to handle strong motions or complex variations, therefore have limited performance on real videos. We first identify the cause of the conflict between style transfer and temporal consistency, and propose to reconcile this contradiction by relaxing the objective function, so as to make the stylization loss term more robust to motions. Through relaxation, style transfer is more robust to inter-frame variation without degrading the subjective effect. Based on relaxation and regularization, we design a zero-shot video style transfer framework. Moreover, for better feature migration, we introduce a new module to dynamically adjust inter-channel distributions. Quantitative and qualitative results demonstrate the superiority of our method over state-of-the-art style transfer methods. </p>

<!-- 	<p class='p2'> Temporal Consistency via Training on Single-Frame </p> 

	<p class='p1'> We mathematically model temporal consistency maintenance as mapping, from which a new regularization is derived. For long-term temporal consistency, we propose a strategy of sharing global features. </p>

	<div align="center">
		<img src="compound_results.jpg" width=90%> <br>
	</div>
	<p class='p1'> Figure 2. Performance of temporal consistency and stylization. Each data point represents an individual experiment. The strength of regularization is represented by different colors. A deeper color indicates a higher temporal loss weight. For the convenience of comparison, we additionally draw some light gray dotted lines. The proposed compound regularization has the best trade-off rates.
	</p>


	<p class='p2'> Consistent Video Style Transfer </p> 
	<div align="center">
		<img src="Architecture.jpg" width=100%> <br>
	</div>
	<p class='p1'> Figure 3. Left: the proposed decorator block for inter-channel feature adjustment. Both target style features and input content features are fed into a shallow sub-network Filter Predictor to predict filters. Residual learning and dimensionality reduction are used to improve the efficiency. Right: The overall architecture of the proposed encoder-decoder style transfer network. </p>

	<p class='p2'> Resources </p> 
	<p class='p1'>
		<ul style="line-height:15px">
 		　　<li> Paper: <a href="" class="links"> Coming Soon </a> </li>
		　　<li> Code: <a href="" class="links"> Coming Soon </a> </li>
		</ul>
	</p> -->

<!-- 	<p class='p2'> Citation</p>
	<p> @InProceedings{Compound2020, <br>
		&nbsp; &nbsp; author = {Wang, Wenjing and Xu, Jizheng and Zhang, Li and Wang, Yue and Liu, Jiaying}, <br>
		&nbsp; &nbsp; title = {Consistent Video Style Transfer via Compound Regularization}, <br>
		&nbsp; &nbsp; booktitle = {AAAI Conference on Artificial Intelligence}, <br>
	   	&nbsp; &nbsp; month = {February}, <br>
	   	&nbsp; &nbsp; year = {2020} <br>
	} <br> 
	</p> -->

<!-- 	<p class='p2'> Selected Results </p> 
	<div align="center">
		<img src="obj_results.jpg" width="860px">
	</div>
	<p class='p1'> Figure 3. Quantitative evaluation of temporal consistency. 
	For the proposed method, Baseline denotes the proposed image style transfer network, Blind denotes using Blind [1] for post-processing, Lt denotes training with temporal loss, and Global denotes using global feature sharing. Our models yields the lowest temporal loss for all temporal length.</p>

	<p class='p2'> Video Results</p>
	<div align="center">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/SawtsxZgSss" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</div>

	<p class='p2'> Reference</p>
	<p class='p1'> [1] Lai, W.-S.; Huang, J.-B.; Wang, O.; Shechtman, E.; Yumer, E.; and Yang, M.-H. 2018. Learning blind video temporal consistency. In Proc. European Conf. Computer Vision, 170–185.</p>
 -->
	<p class='p2'> Related Project</p>
	<p class='p1'> <a href="https://daooshee.github.io/CompoundVST/"> Consistent Video Style Transfer via Compound Regularization</a> </p>


</html>
